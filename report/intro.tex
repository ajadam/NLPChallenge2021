\chapter{Introduction - Présentation du concours}

Le présent rapport a été rédigé dans le cadre de la cinquième édition du \href{https://www.kaggle.com/c/defi-ia-insa-toulouse}{Défi IA organisé par l’INSA Toulouse}. Le défi, rassemblant un large panel d’universités de France et de la francophonie, confronte les équipes d’étudiants de ces universités autour de problématiques pouvant être résolues par des méthodes d’apprentissage machine.
\newline

Tout comme deux autres organismes universitaires de la région, l’Université Rennes II a pris part à ce concours inter-universitaire en y engageant les étudiants en deuxième année du Master de Mathématique Appliqué, Statistiques (MAS) dont nous faisons partie. Cet évènement intervient dans le processus de formation de l’université rennaise.
\newline

Antoine Adam, Dylan Monfret et Loïc Rakotoson formons l’équipe "\emph{We Tried} ", avec l’appui des enseignants-chercheurs et encadrant pour ce projet Yann Soullard et Romain Tavenard, pour répondre à la problématique de cette année.
\newline

\section{Présentation générale}

Pour cette cinquième édition du Défi IA, l’objectif est d’assigner \textbf{automatiquement et le plus convenablement possible un métier à une description de métiers}. Dans le cadre de ce projet, nous dénombrons 28 métiers assignables. Les intitulés de métiers sont tous issues de données récoltées par \href{https://en.wikipedia.org/wiki/Common_Crawl}{Common Crawl}, organisation à but non lucratif recueillant de nombreuses données trouvables sur le web pour les mettre à la disposition de la communauté scientifique. Ces données ont été utilisé lors de la conception du modèle de traitement du langage auto-régressif \href{https://fr.wikipedia.org/wiki/GPT-3}{GPT-3} (plus précisément l’entraînement de celui-ci). Le Défi IA de cette année relève donc à la fois d’un problème de classification en classe multiple et de traitement naturel du langage.
\newline

Cette compétition inclus également quelques contraintes concernant la construction des classifieurs :

\begin{itemize}
\item 217 197 intitulés labellisés servent à la phase apprentissage, 54 300 non labellisés (sans métier associé) pour la phase validation de nos algorithmes et pour les soumissions.
\item Seules les données fournies peuvent être utilisé dans le cadre du concours.
\item Et par conséquent, l’ajustement de modèles pré-entraînés doit se faire uniquement avec les données mises à disposition.
\end{itemize}

\hfill

Les équipes participantes sont classées dans un premier temps sur le F1-Score des prédictions effectué avec le jeu de validation ; puis les 10 meilleures équipes du classement précédent sont jugées sur l’impartialité de leur soumission vis-à-vis du genre. Et c'est donc ici qu’intervient une métrique quantifiant l’impartialité d’un algorithme : le Disparate Impact. 

\section{Variable sensible \& biais de classifieur}

\subsection{Introduction du Disparate Impact} \hfill
\newline

L’équité à l’embauche au regard du genre est une problématique courante dans le monde du travail, et l’introduction de tri préliminaire par apprentissage machine dans le cadre d’embauche n'est pas encore parfaitement au point (comme par exemple le cas des photographies sur les CV pour les personnes de couleurs). Construire un classifieur juste est donc nécessaire. 

Nous concernant, un classifieur parfaitement juste serait en mesure de classer les textes donnés sans prise en compte du genre. C'est évidement impossible puisque certains intitulés décrivent des individus, des hommes ou femmes, et sont donc genrés. Avec les professions qui connaissent de fortes disparités de genre (pour diverse raison d’ordre sociologique), il serait risqué de construire des classifieurs implicitement influencés par le genre. Il faut donc trouver un équilibre entre équité et réalité de chaque métier vis-à-vis du genre. C’est là qu’intervient le Disparate Impact.
\newline

Le \textbf{Disparate Impact} est une métrique qui mesure, comme son nom l’indique, la disparité d’un groupe par rapport à une variable sensible. Pour un groupe donné, le \textbf{DI} est égal au rapport entre la proportion d’individu d’un sous-groupe majoritaire sur la proportion d’un sous-groupe minoritaire. C’est une mesure qui oscille donc entre 1 et l'infini, en supposant que les sous-groupes sont de taille non-nuls (cette métrique n'aurait de toute manière pas d'intérêt dans le cas contraire). Par conséquent, le DI vaut 1 s’il dans les proportions des dits sous-groupes sont équivalents.
\newline

Avec les classifieurs, nous pouvons approcher cette métrique d’un point de vue probabiliste, et ainsi, pour un classifieur $g$, un ensemble de données $X$, une variable cible $Y$ prenant $k$-modalités possibles $\{y_1, \dots , y_k\}$ et $S$ comme variable sensible prenant ici 2 modalités ($0$ pour le groupe majoritaire ou $1$ pour le minoritaire), le DI se définit de la sorte :
\newline

\begin{itemize}
\item Pour le classifieur et une modalité $y_i$ de $Y$ :
$$DI(g, X, S) = \frac{\mathbb{P}(g(X) = y_i | S=0)}{\mathbb{P}(g(X) = y_i | S=1)}$$ 

\item Sur les données réelles et une modalité $y_i$ de $Y$ (biais du jeu de données) :
$$DI(y_i, X, S) = \frac{\mathbb{P}(Y = y_i | S=0)}{\mathbb{P}(Y = y_i | S=1)}$$
\end{itemize}

\hfill

Maintenant, puisque l'on traite près d'une trentaine de classe de $Y$ pour notre problème, on s'intéresse plus au \textbf{macro DI}, soit la moyenne des DI pour toutes les modalités possibles $\{y_1, \dots, y_k\} :$

$$
MDI_{g} = \frac{1}{k} \sum_{i=1}^{k} DI(g, X, S)
$$

$$
MDI_{Y} = \frac{1}{k} \sum_{i=1}^{k} DI(y_i, X, S)
$$

Mais comme dit précédemment, il y a un juste milieu à trouver entre l’équité et le biais "naturel" des données. Le biais du jeu de données ne doit pas être altérer. Donc l’objectif n’est pas d’arriver à $MDI_{g} = 1$ de disparité moyenne, mais de réduire au minimum la différence absolue entre $MDI_{g}$ et $MDI_{Y}$. Le classifieur le plus juste serait donc celui minimisant la différence suivante :

$$ \Delta_{MDI} = | MDI_{g}- MDI_{Y} | $$

\subsection{Le Disparate Impact : un faux problème ?} \hfill
\newline

Une quantité $\Delta_{MDI}$ qui tend vers 0 impliquerait donc un classifieur juste. Mais, nous verrons au cours de l'étude que l'impartialité n'est pas l'objectif immédiat dans la construction de nos classifieurs, nous dirons que c'est un critère que nous surveillons pour savoir si nos classifieurs sont beaucoup trop partiales. Il y la plusieurs raison à ce choix d'orientation :

\begin{itemize}
\item Premièrement, l'objectif du concours qu'est d'intégrer un TOP 10 basé sur le F1-Score des soumissions. La priorité est donc de pousser le F1-Score toujours plus proche de 1.
\item Ensuite, dans le cadre du concours, il nous a été assuré par les organisateurs que les vrais disparités pour chaque métier ne sont pas sensiblement éloignés des disparités des données d'apprentissage. Cela signifie qu'en prédisant le plus justement possible sur le terrain du F1-Score, les DI obtenus pour chaque métier ne seront pas trop éloigné de la réalité lors de nous soumissions sur Kaggle.
\end{itemize}

\hfill

Maintenant, le sujet des disparités de genre au sein de chaque métier n'as pas été totalement écarté. La disparité sera traitée comme \textbf{un problème de données déséquilibrées} avec une partie de \textbf{d'augmentation de données} expliquée plus en détail dans la section \emph{Méthodologie}.


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: master
%%% End: 