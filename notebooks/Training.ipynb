{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from utils.models import (\n",
    "    RobertaClassifier,\n",
    "    ElectraClassifierPL,\n",
    "    T5Generator,\n",
    ")\n",
    "\n",
    "from utils.metrics import Report, Save\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Données\n",
    "Les données après pré-traitement sont disponibles [sur ce lien](https://drive.google.com/drive/folders/1QyPvtM-cVdtwztnyWsLMFQAT8Bejv0nd?usp=sharing):\n",
    "\n",
    "- `test_data`: les données pour la soumission\n",
    "- `train_base_data`: les données fournies lors de la compétition\n",
    "- `dev_data`: les données de validation (Hold-out)\n",
    "- `train_aug_data`: les données d'entraînement augmentées par BBT et Gender Swap\n",
    "- `train_lda_data`: les données d'entraînement avec le label 29\n",
    "- `train_aug_lda_data`: les données d'entraînement augmentées avec le label 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/\"\n",
    "\n",
    "train = pd.read_json(data_path + \"train_aug_data.json\")\n",
    "test = pd.read_json(data_path + \"dev_data.json\")\n",
    "\n",
    "train.Category = train.Category.astype(np.int16)\n",
    "test.Category = test.Category.astype(np.int16)\n",
    "\n",
    "train, val = train_test_split(\n",
    "    train, test_size = .2,\n",
    "    random_state = 42069,\n",
    "    stratify = train.Category\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RoBERTa\n",
    "Exemple d'entrainement pour RoBERTa  \n",
    "Les poids sont disponibles [sur ce lien](https://drive.google.com/drive/folders/1-jiZxzFmozyexvm3vcRVxJTNb-qnLVM8?usp=sharing):\n",
    "\n",
    "- `robLnli_al`: RoBERTa NLI sur les données augmentées\n",
    "- `robL_al`: RoBERTa Large sur les données augmentées\n",
    "\n",
    "Pour entraîner d'autre seed (pour seed averaging), changer la variable `SEED`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Steps: 2803\n",
      "Total Validation Steps: 701\n"
     ]
    }
   ],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 64\n",
    "MAX_LEN = 190\n",
    "N_LABELS = 28\n",
    "BUFFER = 300000\n",
    "SEED = 1\n",
    "MODEL_N = \"robLnli_al\"\n",
    "NTRAIN = train.shape[0]\n",
    "NVAL = val.shape[0]\n",
    "STEPS = int(np.ceil(NTRAIN/BATCH_SIZE))\n",
    "VAL_STEPS = int(np.ceil(NVAL/BATCH_SIZE))\n",
    "\n",
    "print(\"Total Steps:\", STEPS)\n",
    "print(\"Total Validation Steps:\", VAL_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing RobertaClassifier.\n",
      "\n",
      "All the layers of RobertaClassifier were initialized from the model checkpoint at cache/robLnli_al.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaClassifier for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaClassifier.from_pretrained(f\"cache/{MODEL_N}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"cache/{MODEL_N}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition de la fonction de perte, de la métrique et de l'optimiseur.  \n",
    "Compilation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = False)\n",
    "optimizer = tf.keras.optimizers.Adam(lr = 1.5e-5)\n",
    "metrics = tf.keras.metrics.SparseCategoricalAccuracy(name = 'accuracy')\n",
    "\n",
    "model.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss = loss,\n",
    "    metrics = [metrics]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mise en forme des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tokenizer.batch_encode_plus(\n",
    "    train.description.to_list(),\n",
    "    truncation=True,\n",
    "    return_tensors='tf',\n",
    "    max_length=MAX_LEN,\n",
    "    return_attention_mask=False,\n",
    "    padding=\"max_length\")['input_ids']\n",
    "\n",
    "x_val = tokenizer.batch_encode_plus(\n",
    "    val.description.to_list(),\n",
    "    truncation=True,\n",
    "    return_tensors='tf',\n",
    "    max_length=MAX_LEN,\n",
    "    return_attention_mask=False,\n",
    "    padding=\"max_length\")['input_ids']\n",
    "\n",
    "x_test = tokenizer.batch_encode_plus(\n",
    "    test.description.to_list(),\n",
    "    truncation=True,\n",
    "    return_tensors='tf',\n",
    "    max_length=MAX_LEN,\n",
    "    return_attention_mask=False,\n",
    "    padding=\"max_length\")['input_ids']\n",
    "\n",
    "######################################################\n",
    "y_train = K.constant(train.Category, dtype = tf.int32)\n",
    "y_val = K.constant(val.Category, dtype = tf.int32)\n",
    "y_test = K.constant(test.Category, dtype = tf.int32)\n",
    "\n",
    "######################################################\n",
    "train_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_train, y_train))\n",
    "    .repeat()\n",
    "    .shuffle(BUFFER)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "val_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_val, y_val))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices(x_test)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition des callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = Save(path = f\"weights/{MODEL_N}/\", monitor = \"val_loss\")\n",
    "early = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    patience = 2,\n",
    "    restore_best_weights = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entraînement avec $\\alpha = 1.5e-5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['roberta_classifier/roberta/pooler/dense/kernel:0', 'roberta_classifier/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['roberta_classifier/roberta/pooler/dense/kernel:0', 'roberta_classifier/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['roberta_classifier/roberta/pooler/dense/kernel:0', 'roberta_classifier/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['roberta_classifier/roberta/pooler/dense/kernel:0', 'roberta_classifier/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   6/2803 [..............................] - ETA: 15:41 - loss: 3.7423 - accuracy: 0.0910WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0059s vs `on_train_batch_end` time: 16.3146s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0059s vs `on_train_batch_end` time: 16.3146s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2803/2803 [==============================] - 1180s 368ms/step - loss: 0.7711 - accuracy: 0.7822 - val_loss: 0.4574 - val_accuracy: 0.8657\n",
      "Epoch 2/20\n",
      "2803/2803 [==============================] - 1008s 360ms/step - loss: 0.4117 - accuracy: 0.8730 - val_loss: 0.4382 - val_accuracy: 0.8735\n",
      "Epoch 3/20\n",
      "2803/2803 [==============================] - 1008s 360ms/step - loss: 0.3119 - accuracy: 0.9035 - val_loss: 0.4234 - val_accuracy: 0.8810\n",
      "Epoch 4/20\n",
      "2803/2803 [==============================] - 1008s 360ms/step - loss: 0.2355 - accuracy: 0.9280 - val_loss: 0.4635 - val_accuracy: 0.8840\n",
      "Epoch 5/20\n",
      "2803/2803 [==============================] - 1010s 360ms/step - loss: 0.1754 - accuracy: 0.9425 - val_loss: 0.4631 - val_accuracy: 0.8863\n"
     ]
    }
   ],
   "source": [
    "epochs_done = 0\n",
    "history = model.fit(\n",
    "  train_dataset,\n",
    "  epochs = 20,\n",
    "  steps_per_epoch = STEPS,\n",
    "  callbacks = [save, early],\n",
    "  validation_data = val_dataset,\n",
    "  initial_epoch = epochs_done,\n",
    "  workers = 8,\n",
    "  use_multiprocessing = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrainement avec $\\alpha = 1e-6$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['roberta_classifier/roberta/pooler/dense/kernel:0', 'roberta_classifier/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['roberta_classifier/roberta/pooler/dense/kernel:0', 'roberta_classifier/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['roberta_classifier/roberta/pooler/dense/kernel:0', 'roberta_classifier/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['roberta_classifier/roberta/pooler/dense/kernel:0', 'roberta_classifier/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   6/2803 [..............................] - ETA: 15:48 - loss: 0.2670 - accuracy: 0.9184WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0062s vs `on_train_batch_end` time: 16.6981s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0062s vs `on_train_batch_end` time: 16.6981s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2803/2803 [==============================] - 1186s 369ms/step - loss: 0.1909 - accuracy: 0.9407 - val_loss: 0.4254 - val_accuracy: 0.8892\n",
      "Epoch 7/20\n",
      "2803/2803 [==============================] - 1011s 361ms/step - loss: 0.1582 - accuracy: 0.9504 - val_loss: 0.4270 - val_accuracy: 0.8898\n",
      "Epoch 8/20\n",
      "2803/2803 [==============================] - 1012s 361ms/step - loss: 0.1395 - accuracy: 0.9568 - val_loss: 0.4387 - val_accuracy: 0.8904\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(lr = 1e-6)\n",
    "model.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss = loss,\n",
    "    metrics = [metrics]\n",
    ")\n",
    "\n",
    "epochs_done = 5\n",
    "history = model.fit(\n",
    "  train_dataset,\n",
    "  epochs = 20,\n",
    "  steps_per_epoch = STEPS,\n",
    "  callbacks = [save, early],\n",
    "  validation_data = val_dataset,\n",
    "  initial_epoch = epochs_done,\n",
    "  workers = 8,\n",
    "  use_multiprocessing = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tous les poids enregistrés lors de cet entraînement se trouvent [sur ce lien](https://drive.google.com/drive/folders/1-jiZxzFmozyexvm3vcRVxJTNb-qnLVM8?usp=sharing)  \n",
    "Les poids pour la soumission commencent par `PROD-*`\n",
    "\n",
    "Prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing RobertaClassifier.\n"
     ]
    }
   ],
   "source": [
    "predictions = list()\n",
    "for dir_ in glob(\"weights/robLnli_al/*/\"):\n",
    "    model = RobertaClassifier.from_pretrained(dir_)\n",
    "    pred = model.predict(test_dataset)\n",
    "    predictions.append(pred)\n",
    "    \n",
    "y_pred = np.argmax(sum(predictions), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'entraînement d'`ElectraClassifierPL` s'effectue de la même manière que `RobertaClassifier`.\n",
    "\n",
    "## T5 Generator\n",
    "Comme pour `RoBERTa` les poids que nous avons entraîné pour `T5` se trouvent [sur ce lien](https://drive.google.com/drive/folders/16qs2CGR_BXv32MkVIE93lCIaiIBOkWRp?usp=sharing):\n",
    "\n",
    "- `t5S_al`: pour 60 millions de paramètres\n",
    "- `t5B_al`: pour 220 millions de paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 48\n",
    "SEED = 1\n",
    "MODEL_N = \"t5B_al\"\n",
    "STEPS = int(np.ceil(NTRAIN/BATCH_SIZE))\n",
    "VAL_STEPS = int(np.ceil(NVAL/BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing T5Generator.\n",
      "\n",
      "All the layers of T5Generator were initialized from the model checkpoint at cache/t5B_al.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5Generator for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = T5Generator.from_pretrained(f\"cache/{MODEL_N}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"cache/{MODEL_N}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition de la métrique et de l'optimiseur.  \n",
    "Redéfinition de `save`  \n",
    "Compilation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = Save(path = f\"weights/{MODEL_N}/\", monitor = \"val_loss\")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr = 1.5e-5)\n",
    "metrics = tf.keras.metrics.SparseTopKCategoricalAccuracy(name = 'accuracy')\n",
    "\n",
    "model.compile(optimizer = optimizer, metrics=[metrics])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction de formatage de données pour notre modèle T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(df):\n",
    "    \"\"\"\n",
    "    Format data for T5 Generator\n",
    "    df: DataFrame with description and Category names\n",
    "    :return: Tensor dataset\n",
    "    \"\"\"\n",
    "    x = df.description.to_list()\n",
    "    y = df.Category.to_list()\n",
    "    x = tokenizer.batch_encode_plus(\n",
    "      x, truncation=True,\n",
    "      return_attention_mask = False,\n",
    "      return_tensors='tf', max_length=MAX_LEN,\n",
    "      padding = \"max_length\")\n",
    "    y = tokenizer.batch_encode_plus(\n",
    "      y, truncation=True,\n",
    "      return_attention_mask = False,\n",
    "      return_tensors='tf', max_length=4,\n",
    "      padding = \"max_length\")\n",
    "    data = {\n",
    "        'input_ids': x['input_ids'],\n",
    "        'labels': y['input_ids']\n",
    "    }\n",
    "    dataset = (\n",
    "      tf.data.Dataset\n",
    "      .from_tensor_slices(data)\n",
    "      .shuffle(BUFFER)\n",
    "      .batch(BATCH_SIZE)\n",
    "      .prefetch(AUTO)\n",
    "      .repeat()\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "###########################################\n",
    "train_dataset = generate_dataset(train)\n",
    "val_dataset = generate_dataset(val)\n",
    "\n",
    "x_test = tokenizer.batch_encode_plus(\n",
    "    test.description.to_list(), truncation=True, \n",
    "    return_tensors='tf', max_length=MAX_LEN,\n",
    "    return_attention_mask = False,\n",
    "    padding = \"max_length\")['input_ids']\n",
    "\n",
    "y_test = test.Category.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entraînement avec $\\alpha = 1.5e-5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3737/3737 [==============================] - 4447s 1s/step - accuracy: 0.9265 - loss: 0.3461 - lr: 1.5000e-05 - val_accuracy: 0.9922 - val_loss: 0.1547\n",
      "Epoch 2/20\n",
      "3737/3737 [==============================] - 4400s 1s/step - accuracy: 0.9917 - loss: 0.1574 - lr: 1.5000e-05 - val_accuracy: 0.9933 - val_loss: 0.1353\n",
      "Epoch 3/20\n",
      "3737/3737 [==============================] - 4410s 1s/step - accuracy: 0.9929 - loss: 0.1395 - lr: 1.5000e-05 - val_accuracy: 0.9938 - val_loss: 0.1275\n",
      "Epoch 4/20\n",
      "3737/3737 [==============================] - 4396s 1s/step - accuracy: 0.9937 - loss: 0.1284 - lr: 1.5000e-05 - val_accuracy: 0.9941 - val_loss: 0.1226\n",
      "Epoch 5/20\n",
      "3737/3737 [==============================] - 4406s 1s/step - accuracy: 0.9944 - loss: 0.1199 - lr: 1.5000e-05 - val_accuracy: 0.9943 - val_loss: 0.1172\n",
      "Epoch 6/20\n",
      "1460/3737 [==========>...................] - ETA: 41:23 - accuracy: 0.9949 - loss: 0.1139 - lr: 1.5000e-05"
     ]
    }
   ],
   "source": [
    "epochs_done = 0\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs = 20,\n",
    "    steps_per_epoch = STEPS,\n",
    "    callbacks = [save, early],\n",
    "    validation_data = val_dataset,\n",
    "    validation_steps = VAL_STEPS,\n",
    "    initial_epoch = epochs_done,\n",
    "    workers = 8,\n",
    "    use_multiprocessing = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tous les poids enregistrés lors de cet entraînement se trouvent [sur ce lien](https://drive.google.com/drive/folders/1-q6fTOJmEVtETv30JqbiIHTiS9UDJFWm?usp=sharing)\n",
    "\n",
    "Prédiction (ou plutôt génération)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing T5Generator.\n"
     ]
    }
   ],
   "source": [
    "predictions = list()\n",
    "for dir_ in glob(\"weights/t5B_al/*/\"):\n",
    "    model = T5Generator.from_pretrained(dir_)\n",
    "    pred = model.batch_generate(x_test, 128)\n",
    "    pred = tokenizer.batch_decode(pred.numpy().tolist())\n",
    "    predictions.append(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
